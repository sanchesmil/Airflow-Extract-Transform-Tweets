{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurar o ambiente interativo do PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria a sessao spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/14 16:34:16 WARN Utils: Your hostname, MI0118G-ATIC resolves to a loopback address: 127.0.1.1; using 172.29.249.25 instead (on interface eth0)\n",
      "23/11/14 16:34:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/14 16:34:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/14 16:34:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/pedro/projetos/projetos_python/proj_twitter_pipeline/src/notebooks/exploracao_spark.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro/projetos/projetos_python/proj_twitter_pipeline/src/notebooks/exploracao_spark.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro/projetos/projetos_python/proj_twitter_pipeline/src/notebooks/exploracao_spark.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m.\u001b[39;49mbuilder\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro/projetos/projetos_python/proj_twitter_pipeline/src/notebooks/exploracao_spark.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mtwitter_transformation\u001b[39;49m\u001b[39m\"\u001b[39;49m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/pedro/projetos/projetos_python/proj_twitter_pipeline/src/notebooks/exploracao_spark.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/projetos/projetos_python/proj_twitter_pipeline/venv/lib/python3.9/site-packages/pyspark/sql/session.py:272\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    270\u001b[0m     \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_options)\n\u001b[1;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    275\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/projetos/projetos_python/proj_twitter_pipeline/venv/lib/python3.9/site-packages/pyspark/sql/session.py:307\u001b[0m, in \u001b[0;36mSparkSession.__init__\u001b[0;34m(self, sparkContext, jsparkSession, options)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    304\u001b[0m             jsparkSession, options\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m         jsparkSession \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSparkSession(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), options)\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mgetattr\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mapplyModifiableSettings(\n\u001b[1;32m    310\u001b[0m         jsparkSession, options\n\u001b[1;32m    311\u001b[0m     )\n",
      "File \u001b[0;32m~/projetos/projetos_python/proj_twitter_pipeline/venv/lib/python3.9/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1586\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   1588\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/projetos/projetos_python/proj_twitter_pipeline/venv/lib/python3.9/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:179)\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:196)\n\tat py4j.Gateway.invoke(Gateway.java:237)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"twitter_transformation\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define um dataframe para o caminho do DataLake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"../../datalake/twitter_datascience\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+------------+\n",
      "|                data|            includes|              meta|extract_date|\n",
      "+--------------------+--------------------+------------------+------------+\n",
      "|[{2, 100, 2023-11...|{[{2023-11-02T04:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{72, 6, 2023-11-...|{[{2023-11-02T15:...|              null|  2023-11-02|\n",
      "|[{0, 82, 2023-11-...|{[{2023-11-02T11:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{11, 69, 2023-11...|{[{2023-11-02T07:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{59, 9, 2023-11-...|{[{2023-11-02T16:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{4, 80, 2023-11-...|{[{2023-11-02T02:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{11, 99, 2023-11...|{[{2023-11-02T15:...|              null|  2023-11-02|\n",
      "|[{8, 37, 2023-11-...|{[{2023-11-02T17:...|              null|  2023-11-02|\n",
      "|[{75, 65, 2023-11...|{[{2023-11-02T18:...|              null|  2023-11-02|\n",
      "|[{56, 72, 2023-11...|{[{2023-11-02T21:...|              null|  2023-11-02|\n",
      "|[{32, 25, 2023-11...|{[{2023-11-02T05:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{83, 13, 2023-11...|{[{2023-11-02T18:...|              null|  2023-11-02|\n",
      "|[{45, 93, 2023-11...|{[{2023-11-02T21:...|              null|  2023-11-02|\n",
      "|[{50, 63, 2023-11...|{[{2023-11-02T23:...|              null|  2023-11-02|\n",
      "|[{7, 21, 2023-11-...|{[{2023-11-02T23:...|              null|  2023-11-02|\n",
      "|[{74, 75, 2023-11...|{[{2023-11-02T04:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{59, 52, 2023-11...|{[{2023-11-02T01:...|{1234567890abcdef}|  2023-11-02|\n",
      "|[{21, 89, 2023-11...|{[{2023-11-02T10:...|              null|  2023-11-02|\n",
      "|[{30, 89, 2023-11...|{[{2023-11-02T00:...|              null|  2023-11-02|\n",
      "|[{92, 65, 2023-11...|{[{2023-11-02T10:...|{1234567890abcdef}|  2023-11-02|\n",
      "+--------------------+--------------------+------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- author_id: string (nullable = true)\n",
      " |    |    |-- conversation_id: string (nullable = true)\n",
      " |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |-- edit_history_tweet_ids: array (nullable = true)\n",
      " |    |    |    |-- element: long (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |    |-- lang: string (nullable = true)\n",
      " |    |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      " |-- includes: struct (nullable = true)\n",
      " |    |-- users: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- created_at: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- username: string (nullable = true)\n",
      " |-- meta: struct (nullable = true)\n",
      " |    |-- next_token: string (nullable = true)\n",
      " |-- extract_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col: struct (nullable = true)\n",
      " |    |-- author_id: string (nullable = true)\n",
      " |    |-- conversation_id: string (nullable = true)\n",
      " |    |-- created_at: string (nullable = true)\n",
      " |    |-- edit_history_tweet_ids: array (nullable = true)\n",
      " |    |    |-- element: long (containsNull = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- in_reply_to_user_id: string (nullable = true)\n",
      " |    |-- lang: string (nullable = true)\n",
      " |    |-- public_metrics: struct (nullable = true)\n",
      " |    |    |-- like_count: long (nullable = true)\n",
      " |    |    |-- quote_count: long (nullable = true)\n",
      " |    |    |-- reply_count: long (nullable = true)\n",
      " |    |    |-- retweet_count: long (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode(\"data\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- conversation_id: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- like_count: long (nullable = true)\n",
      " |-- quote_count: long (nullable = true)\n",
      " |-- reply_count: long (nullable = true)\n",
      " |-- retweet_count: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode(\"data\").alias(\"tweets\"))\\\n",
    "  .select(\"tweets.author_id\", \"tweets.conversation_id\", \\\n",
    "          \"tweets.created_at\", \"tweets.id\", \\\n",
    "          \"tweets.public_metrics.*\", \"tweets.text\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = df.select(f.explode(\"data\").alias(\"tweets\"))\\\n",
    "  .select(\"tweets.author_id\", \"tweets.conversation_id\", \\\n",
    "          \"tweets.created_at\", \"tweets.id\", \\\n",
    "          \"tweets.public_metrics.*\", \"tweets.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|author_id|conversation_id|          created_at| id|like_count|quote_count|reply_count|retweet_count|                text|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "|        2|            100|2023-11-02T07:09:...| 99|        47|         65|         20|           30|Tweet fictício ge...|\n",
      "|       32|              1|2023-11-02T16:31:...| 52|        66|         46|         76|           21|Tweet fictício cr...|\n",
      "|       29|              0|2023-11-02T10:48:...| 94|        53|         62|         49|           61|Tweet fictício ge...|\n",
      "|       53|             81|2023-11-02T00:27:...| 90|        72|         69|         32|           48|Tweet fictício cr...|\n",
      "|       79|             62|2023-11-02T06:42:...| 15|        30|         46|         17|            3|Este é um tweet f...|\n",
      "+---------+---------------+--------------------+---+----------+-----------+-----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(f.explode(\"includes.users\").alias(\"users\")) \\\n",
    "  .select(\"users.*\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = df.select(f.explode(\"includes.users\").alias(\"users\")).select(\"users.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|{2023-11-02T04:05...|\n",
      "|{2023-11-02T02:44...|\n",
      "|{2023-11-02T03:01...|\n",
      "|{2023-11-02T21:57...|\n",
      "|{2023-11-02T01:08...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df_test = df.select(f.explode(\"includes.users\")).select(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 col|\n",
      "+--------------------+\n",
      "|{2023-11-02T04:05...|\n",
      "|{2023-11-02T02:44...|\n",
      "|{2023-11-02T03:01...|\n",
      "|{2023-11-02T21:57...|\n",
      "|{2023-11-02T01:08...|\n",
      "|{2023-11-02T13:33...|\n",
      "|{2023-11-02T20:27...|\n",
      "|{2023-11-02T21:11...|\n",
      "|{2023-11-02T14:44...|\n",
      "|{2023-11-02T07:06...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_df_test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monta a estrutura final que será salva em um arquivo \n",
    "- método .coalesce(1): o parâmetro 1 é responsável por unir as informações que o Spark quebra em pedaços\n",
    "- método .write.mode(\"overwrite\"): o parâmetro utilizado serve para evitar problemas ao encontrar os dados já salvos\n",
    "- método .json('output/tweet'): indica que salvaremos em formato JSON na estrutura de pasta definida no parâmetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.coalesce(1).write.mode(\"overwrite\").json('output/tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.coalesce(1).write.mode(\"overwrite\").json('output/user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
